{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a dataset from the UCI dataset (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) which contains a list if messages which are classified either as spam or not. So we are going to create a model to predict if a given message is spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rstrip() returns a copy of the string with trailing characters stripped.\n",
    "random_string = ' this is good'\n",
    "### Leading whitepsace are removed\n",
    "print(random_string.rstrip()\n",
    "\n",
    "OUTPUT\n",
    "\n",
    "this is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mess_no,message in enumerate(messages[:10]):\n",
    "    print(mess_no,message)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep = '\\t', names = ['label','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " messages['length'] = messages['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x0000025D839F86A0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x0000025D83CE3550>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25d838abcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now from the above diagram we can say that length is a good feature to detect if a message is ham or spam. We can conclude that the average length if the ham message is around 50 and average length for the spam message is around 150. Thereby we could us length of the message to detect if it is ham or spam.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-procession\n",
    "\n",
    "In this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n",
    "\n",
    "let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey  there how are you I have been to California last month '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "import string\n",
    "\n",
    "mess = 'Hey !!! there how are you. I have been to California last month '\n",
    "\n",
    "# Check characters to see if they are in punctuation\n",
    "nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "# Join the characters again to form the string.\n",
    "nopunc = ''.join(nopunc)\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'there',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'to',\n",
       " 'California',\n",
       " 'last',\n",
       " 'month']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just remove any stopwords\n",
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'California', 'last', 'month']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's put both of these together in a function to apply it to our DataFrame later on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's \"tokenize\" these messages. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens (words that we actually want).\n",
    "Let's see an example output on on column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "We'll do that in three steps using the bag-of-words model:\n",
    "\n",
    "1)Count how many times does a word occur in each message (Known as term frequency)\n",
    "\n",
    "2)Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "\n",
    "3)Normalize the vectors to unit length, to abstract from the original text length (L2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# Might take awhile...\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one text message and get its bag-of-words counts as a vector, putting to use our new bow_transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see its vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n",
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there are seven unique words in message number 4 (after removing common stop words). Two of them appear twice, the rest only once. Let's go ahead and check and confirm which ones appear twice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the vector representation to all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 11425)\n",
      "Amount of Non-Zero occurences:  50548\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF (inverse document frequency) of the word `\"u\"` and of word \"university\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2800524267409408\n",
      "8.527076498901426\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.714892143529009\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['call']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115 5572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(messages['message'], messages['label'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run our model and then predict off the test set. We will use SciKit Learn's [pipeline](http://scikit-learn.org/stable/modules/pipeline.html) capabilities to store a pipeline of workflow. This will allow us to set up all the transformations that we will do to the data for future use. Let's see an example of how it works:\n",
    "\n",
    "### Instead of doing all the process from count vectorization to tf-idf calculation we can just create a pipeline and perform all the operations in a single step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_process at 0x0000025D83ED4EA0>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocesso...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.96      0.98      1026\n",
      "       spam       0.68      1.00      0.81        89\n",
      "\n",
      "avg / total       0.97      0.96      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
